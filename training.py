import torch
import torch.nn as nn
import torch.optim as optim


def train_nn(discriminator, generator, training_loader, device):
    # define parameters
    EPOCH = 10
    LR = 0.001
    criterion = nn.BCELoss()
    optimizerD = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))
    optimizerG = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))

    # actual training
    train_nn_helper(generator, discriminator, training_loader, EPOCH, criterion, optimizerD, optimizerG, device)


def train_nn_helper(generator, discriminator, training_loader, EPOCH, criterion, optimizerD, optimizerG, device):
    for epoch in range(EPOCH):
        for i, data in enumerate(training_loader):
            real_image_, _ = data
            real_image = real_image_.clone().detach().to(device)

            # 1st Step: Updating the weights of the neural network of the discriminator
            discriminator.zero_grad()

            # Training the discriminator with a real image of the dataset
            target = torch.ones(real_image.size()[0], device=device)
            output = discriminator(real_image)
            errD_real = criterion(output, target)

            # Training the discriminator with a fake image generated by the generator
            noise = torch.randn(real_image.size()[0], 100, 1, 1, device=device)
            fake = generator(noise)  # already in GPU
            target = torch.zeros(real_image.size()[0], device=device)
            output = discriminator(fake.detach())
            errD_fake = criterion(output, target)

            # Backpropagating the total error
            errD = errD_real + errD_fake
            errD.backward()
            optimizerD.step()

            # 2nd Step: Updating the weights of the neural network of the generator
            generator.zero_grad()
            target = torch.ones(real_image.size()[0], device=device)
            output = discriminator(fake)
            errG = criterion(output, target)
            errG.backward()
            optimizerG.step()


            # # reshape the array to dimensions of: batch x width pixels x height pixels x RGB (for saving purposes)
            # # real_image = real_image.numpy().transpose(0, 2, 3, 1)
            # # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps
            # print('[%d/%d][%d/%d] Loss_D: %.4f; Loss_G: %.4f' % (
            # epoch, EPOCH, i, len(dataloader), errD.item(), errG.item()))
            # if i % 100 == 0:
            #     vutils.save_image(real_image, '%s/real_samples.png' % "./results", normalize=True)
            #     fake = generator(noise)
            #     vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize=True)
